{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen-nhat-mai/methane-leak-detection/blob/main/Web_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-7rHnD4hm6M"
      },
      "source": [
        "# Build web app for classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lSfR_PaYYrLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2646ec3-5215-4dfe-9cb2-b338572c3e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ver 1: Upload and infer image by image\n",
        "\n"
      ],
      "metadata": {
        "id": "wvi_isiIcvUq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPD5lMyRa8Ph"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://towardsdatascience.com/create-an-image-classification-web-app-using-pytorch-and-streamlit-f043ddf00c24#2b4c\n",
        "https://www.youtube.com/watch?v=NEhrkeF2o_M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMtGOJ0FYuDT",
        "outputId": "74bac1c1-4494-4e07-b4a9-f7b6d606f859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "# Write the web app\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "import torch\n",
        "\n",
        "#---------------------- TO UPDATE WITH FINAL MODEL-----------------------#\n",
        "def predict(image_path):\n",
        "    # Load model\n",
        "    best_model = models.resnet101(pretrained=True)\n",
        "    # Define transformation\n",
        "    transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "      mean=[0.485, 0.456, 0.406],\n",
        "      std=[0.229, 0.224, 0.225]\n",
        "      )])\n",
        "    # Load data\n",
        "    img = Image.open(image_path)\n",
        "    batch_t = torch.unsqueeze(transform(img), 0)\n",
        "    # Do inference\n",
        "    best_model.eval()\n",
        "    out = best_model(batch_t)\n",
        "    # Load all of the classes => Yes/ No for methane case\n",
        "    with open('imagenet_classes.txt') as f:\n",
        "        classes = [line.strip() for line in f.readlines()]\n",
        "    # Calculate the probability\n",
        "    prob = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    _, indices = torch.sort(out, descending=True)\n",
        "    # Return the classes and corresponding probability\n",
        "    return [(classes[idx], prob[idx].item()) for idx in indices[0][:2]]\n",
        "\n",
        "# ------------------------------WEB APP----------------------------------#\n",
        "st.set_option('deprecation.showfileUploaderEncoding', False)\n",
        "\n",
        "st.title(\"Localize Methane Leaks\")\n",
        "st.write(\"\")\n",
        "\n",
        "file_up = st.file_uploader(\"Upload an image\", type=\"jpg\")\n",
        "\n",
        "if file_up is not None:\n",
        "    image = Image.open(file_up)\n",
        "    st.image(image, caption='Uploaded Image.', use_column_width=True)\n",
        "    st.write(\"\")\n",
        "    st.write(\"Just a second...\")\n",
        "    labels = predict(file_up)\n",
        "\n",
        "    # print out the prediction labels with scores\n",
        "    for i in labels:\n",
        "        st.write(\"Prediction:\", i[0], \",   Probability (%): \", i[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1-u_S0Vbltc",
        "outputId": "c2a9637b-0c1e-4db7-b15c-d2d137fdeb28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[#######...........] / extract:localtunnel: verb lock using /root/.npm/_locks/s\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.143.211:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.458s\n",
            "your url is: https://chatty-tigers-relate.loca.lt\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Run app.py and made available on a local URL\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ver 2: Upload batch of images and export results"
      ],
      "metadata": {
        "id": "6cuHW-O5c7VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the web app\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "import torch\n",
        "\n",
        "#---------------------- TO UPDATE WITH FINAL MODEL-----------------------#\n",
        "def predict(image_path):\n",
        "    # Load model\n",
        "    best_model = models.resnet101(pretrained=True)\n",
        "    # Define transformation\n",
        "    transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "      mean=[0.485, 0.456, 0.406],\n",
        "      std=[0.229, 0.224, 0.225]\n",
        "      )])\n",
        "    # Load data\n",
        "    img = Image.open(image_path)\n",
        "    batch_t = torch.unsqueeze(transform(img), 0)\n",
        "    # Do inference\n",
        "    best_model.eval()\n",
        "    out = best_model(batch_t)\n",
        "    # Load all of the classes => Yes/ No for methane case\n",
        "    with open('imagenet_classes.txt') as f:\n",
        "        classes = [line.strip() for line in f.readlines()]\n",
        "    # Calculate the probability\n",
        "    prob = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    _, indices = torch.sort(out, descending=True)\n",
        "    # Return the classes and corresponding probability\n",
        "    return [(classes[idx], prob[idx].item()) for idx in indices[0][:2]]\n",
        "\n",
        "# ------------------------------WEB APP----------------------------------#\n",
        "st.set_option('deprecation.showfileUploaderEncoding', False)\n",
        "\n",
        "st.title(\"Localize Methane Leaks\")\n",
        "st.write(\"\")\n",
        "\n",
        "file_up = st.file_uploader(\"Upload your images\", type=[\"jpg\", \"jpeg\", \"png\"], accept_multiple_files=True)\n",
        "\n",
        "if file_up is not None:\n",
        "    for img_file in file_up:\n",
        "        image = Image.open(img_file)\n",
        "        st.image(image, caption='Uploaded Image.', use_column_width=True)\n",
        "        st.write(\"\")\n",
        "        st.write(\"Just a second...\")\n",
        "        labels = predict(img_file)\n",
        "\n",
        "        # print out the prediction labels with scores\n",
        "        for i in labels:\n",
        "            st.write(\"Prediction:\", i[0], \",   Probability (%): \", i[1])"
      ],
      "metadata": {
        "id": "wdVDgJt4dEZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run app.py and made available on a local URL\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD5UEGqkd0HL",
        "outputId": "f7e1a44b-2b5e-410a-912b-26bc78361470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.73.128.93:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 5.297s\n",
            "your url is: https://flat-experts-shine.loca.lt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}